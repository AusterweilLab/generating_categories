%!TEX output_directory = latex_out/

\documentclass[10pt,letterpaper]{article}

\usepackage{cogsci}
\usepackage{pslatex}
\usepackage{mathtools}
\usepackage[natbibapa]{apacite}


% set up PGF
\usepackage{pgfplots}
\pgfplotsset{compat=1.13}
\newcommand\inputpgf[2]{{
\let\pgfimageWithoutPath\pgfimage
\renewcommand{\pgfimage}[2][]{\pgfimageWithoutPath[##1]{#1/##2}}
\input{#1/#2}
}}


\title{PACKER: An Exemplar Model of Category Generation}

\author{
{ \large \bf Nolan Conaway (nconaway@wisc.edu) } \\
{ \large \bf Joseph L. Austerweil (austerweil@wisc.edu) } \\
Department of Psychology, 1202 W. Johnson Street \\
Madison, WI 53706 USA
}


\begin{document}

\maketitle

\begin{abstract}

Generating new concepts is an intriguing yet understudied topic in cognitive science. In this paper, we present a novel exemplar model of category generation: PACKER (\textit{Producing Alike and Contrasting Knowledge using Exemplar Representations}). PACKER's core design assumptions are (1) categories are represented as exemplars in a multidimensional psychological space, (2) generated items should be similar to exemplars of the same category, and (3) generated categories should be dissimilar to existing categories. A behavioral study reveals strong effects of contrast- and target-class similarity. These effects are novel empirical phenomena, which are directly predicted by the PACKER model but are not explained by existing formal approaches. 

\textbf{Keywords:} 
Categorization, exemplar models, category generation, creative cognition, computational modeling.
\end{abstract}

\section{Introduction}

The creation of new concepts and ideas is among the most interesting -- yet infrequently studied -- capabilities of human cognition. This paper focuses on one topic within the broader field of creative cognition: category generation. Foundational work on this topic \citep[e.g.,][]{smith1993constraining,ward2002role,ward1994structured,ward1995s} has focused on the role of prior knowledge in generating novel concepts. A core phenomenon is that people generate categories with similar distributional properties as existing categories. For example, \citet{ward1994structured} asked participants to generate species of plants and animals that might exist on other planets. Generation was strongly constrained by prior knowledge of Earth species: People generated species with the same features as those found on Earth (e.g., eyes, legs, wings) and possessing the same feature correlations observed on Earth (e.g., feathers co-occur with wings). 

Recent work has proposed and tested formal models to explain these observations. \citet{jern2013probabilistic} trained participants on experimenter-defined categories composed of exemplars within an artificial three-dimensional domain. After a short training phase, participants were asked to generate exemplars from a new category. Participants were provided with a set of scales to adjust the feature values of each generated stimulus, and were given unlimited time to create each example. As in the classic \cite{ward1994structured} experiment, \citet{jern2013probabilistic} found that generated categories possessed the same feature variance and correlations as the experimenter-defined categories in the domain. 

\citet{jern2013probabilistic} tested several different computational models on their data. Most relevant to the present investigation, they tested a `copy-and-tweak' model that generates items by copying and changing previous observations, and a hierarchical Bayesian model that uses the structure of known categories to infer the structure of new ones. They found that the hierarchical Bayesian model provided the strongest account of the behavioral results.


\begin{figure*}
    \begin{center}
    \inputpgf{figs/}{packer-examples.pgf}
    \caption{PACKER generation of a category `B' example, following exposure to one member of category `A' and category `B'. The panels differ in how the trade-off between within- and between-category similarity is managed (via the $\gamma$ parameter).}
    \label{fig:example-prob-spaces}
    \end{center}
\end{figure*}

In this paper, we introduce a novel exemplar-based approach to category generation, PACKER (\textit{Producing Alike and Contrasting Knowledge using Exemplar Representations}), which creates  categories by balancing two constraints: (1) new categories should be different from known categories (minimizing between-class similarity), and (2) new categories should be internally coherent (maximizing within-class similarity). As such, PACKER is a significant departure from previous accounts of generation -- rather than proposing that people create categories by abstracting and re-using knowledge of related categories, PACKER first considers how the generated category should \textit{differ} from related categories. Further, it does so using the well-studied mechanics of exemplar representations and therefore possesses a rich connection to the wider body of research on category learning.

In the sections below, we formally describe the PACKER model and explore its predictions in a behavioral experiment. We compare its performance to copy-and-tweak and hierarchical Bayesian models by examining their fits to aggregate results and individual differences.

\section{PACKER: An Exemplar Model}

The PACKER model is an extension of the Generalized Context Model of category learning (GCM; \citealp{nosofsky1984choice}). It assumes that each category is encoded by a set of exemplars within a $k$-dimensional psychological space, and that generation is constrained by both similarity to members of the target category (the category in which a stimulus is being generated) as well as similarity to members of other categories. 

As in the GCM, the similarity between two examples, $s\left(x_i, x_j\right)$, is an inverse-exponential function of distance:
\begin{equation}
  s\left(x_i,x_j\right) = \exp \left\{ -c \sum_{k}{ \left| x_{ik} - x_{jk} \right|}w_k \right\}
  \label{eq:similarity}
\end{equation}
where $w_k$ is the attention weighting of dimension $k$ ($w_k \geq 0$ and $\sum_k{w_k} = 1$), accounting for the relative importance of each dimension in similarity calculations, and $c$ ($c>0$) is a specificity parameter controlling the spread of exemplar generalization. For simplicity, our simulations will use uniform attention weights, except when otherwise noted.  

To generate a new example, the model considers both the similarity to examples from contrast categories as well as the similarity to examples (if any exist) in the target category. The aggregated similarity $a$ between generation candidate $y$ and stored exemplars $x$ is:

\begin{equation}
  a(y, x) = \sum_j{f(x_j) s(y, x_j)}
  \label{eq:aggregation}
\end{equation}
\
where $f(x_j)$ is a function specifying the extent to which each exemplar contributes to the generation. A negative value for $f(x_j)$ produces a `repelling' effect (items are less likely to be generated nearby $x_j$), and a positive value produces an `attracting' effect (items are more likely to be generated nearby $x_j$). When $f(x_j)=0$, the exemplar does not contribute to generation. 

PACKER sets $f(x_j)$ depending on exemplar $j$'s category membership: $f(x_j) = \gamma$ if $x_j$ is a member of the target category, and $f(x_j) = \gamma - 1$ if $x_j$ is a member of a contrast category. $\gamma$ is thus a free parameter ($0 \leq \gamma \leq 1$) controlling the trade-off between within- and between-category similarity. PACKER's core proposal is that new categories should be different from existing categories, and same-category exemplars should be similar to one another. This is realized when $\gamma$ assumes an intermediate value: For example, when $\gamma = 0.5$, $f(x_j) = 0.5$ for members of the target category and $f(x_j) = -0.5$ for members of other categories; thus, the model is likely to generate items that are similar to members of the target category but are not similar to members of other categories. However, more extreme values can be used to produce different behavior, see Figure \ref{fig:example-prob-spaces}.

The probability that a given candidate $y$ will be generated is evaluated using an Exponentiated \citet{luce1977choice} choice rule. Candidates with greater values of $a$ are more likely to be generated than candidates with smaller values:
\begin{equation}
p(y) = \dfrac
    { \exp( { \theta \cdot a(y, x) } ) }
    { \sum_i{ \exp( { \theta \cdot a(y_i, x) } ) } }
    \label{eq:packer-choice}
\end{equation}
where $\theta$ ($\theta \geq 0$) controls response determinism. 

\subsection{Summary}
The proposed PACKER model suggests people generate categories by minimizing between-category similarity and maximizing within-category similarity. The underlying processes assumed by PACKER are highly similar to those in the GCM. The main difference is that PACKER aggregates positive- and negative-valued similarities, rather than only aggregating positive-valued similarities. In later sections, we will explore the unique predictions yielded by these design principles. First, however, we contrast PACKER with other category generation models. 

\section{Previous Accounts of Category Generation}

Previous models of category generation focus on capturing the tendency for people to produce new categories that have similar distributional properties to existing categories. To the best of our knowledge, \citet{jern2013probabilistic} were the first to evaluate computational models of generation. Based on their work, we describe two alternative models: a formalization of the \textit{Path of Least Resistance} hypothesis (later termed {\em copy-and-tweak}, see \citealp{jern2013probabilistic}), and the \textit{hierarchical sampling} hypothesis \citep{jern2013probabilistic}.


\subsection{Copy-and-Tweak}
The \textit{copy-and-tweak} model, based broadly on the earlier Path of Least Resistance view \citep{ward1994structured,ward1995s}, proposes that participants generate categories by retrieving an observation of the target class from memory, and then tweaking it to make something new. \citet{jern2013probabilistic} interpreted this proposal in terms of an exemplar model using the GCM \citep{nosofsky1984choice}. Formally, their model is equivalent to PACKER with $\gamma = 1$ (see Figure \ref{fig:example-prob-spaces}). In this case,  $f(x_j) = 1$ for members of the target category and $f(x_j) = 0$ for members of other categories; thus, the model considers only target-class similarity, and when no members of the target class are known, the model generates items at random. 

In our work we provide simulations from the copy-and-tweak account, realized as a variant of PACKER with a fixed $\gamma$ parameter. Formalizing a model family where PACKER and copy-and-tweak are different parameterizations within the same framework is useful because comparison between the models provides a test of the explanatory value of the contrast mechanism: The account provided by copy-and-tweak will only equal that of PACKER if the contrast mechanism does not offer an advantage (i.e., if $\gamma < 1$ significantly improves model fits).



\subsection{Hierarchical Sampling}
% from other categories 
%with independently generated means, but correlated covariance matrices.
%Observed exemplars (represented as points in this space) are viewed as samples from their underlying category distribution (e.g., a multivariate normal distribution). These category distributions are in turn viewed as samples from an underlying domain distribution, which represents the common distributional characteristics among known categories (e.g., feature variance, feature correlations). When asked to generate a novel class, people are thought to sample a category distribution from the domain distribution, and then sample exemplars from the category distribution. Thus, exemplars generated into novel classes tend to obey the same distributional properties of known categories. 

Based on several results inconsistent with the copy-and-tweak account, \citet{jern2013probabilistic} advocated a hierarchical Bayesian model. Exemplars of each category were generated from a multivariate Normal distribution over the dimensions of stimulus space. The mean of each category was independently generated, but the covariance matrix (encoding feature variances and correlations) was generated from a common prior distribution. New categories are produced by generating a new mean (uniform over stimulus space) and covariance matrix from the common prior distribution. Because the shared prior distribution's parameters were unobserved, a hierarchical Bayesian model uses information from the previous categories (their feature variances and correlations) to generate the covariance matrix of the new category.

Each category's exemplars are assumed to be a multivariate Normal distribution with parameters ($\mu, \Sigma$). Each category's covariance matrix is assumed to be inverse-Wishart distributed with parameters ($v$, $\kappa,$ and $\Sigma_D$).\footnote{Note that \citet{jern2013probabilistic}'s model is slightly different, as they used a non-conjugate model. Their model acts very similar to our version of it and receives comparable fits.} $\Sigma_D$ is the covariance matrix shared between categories. We assume the shared covariance matrix $\Sigma_D$ is generated from a Wishart distribution (for conjugacy) with parameters $v_0$, $\kappa_0$, and $\Sigma_0$. We set $\nu_0 = 4$, and $\Sigma_0 = \lambda {\bf I}$, where $\lambda$ is a free parameter controlling the expected variance of dimensions (dimensions of the shared covariance matrix are expected to be uncorrelated) and ${\bf I}$ is the identity matrix.

To simplify the model predictions, we used {\em maximum a posteriori} (MAP) estimates for the hidden parameters and then generated new categories based on those estimates. Due to conjugacy, the MAP estimate for the shared covariance matrix $\Sigma_D = \Sigma_0 + \sum_c{C_c}$, where $C_c$ is the empirical covariance matrix of category $c$. The MAP estimate of the covariance matrix for the target category $B$ is 
\begin{equation}
  \Sigma_B = \left[ \Sigma_D \nu + C_B +
    \dfrac
    {\kappa n_B}
    {\kappa + n_B}
    (\bar{x_B}-\mu_B)(\bar{x_B}-\mu_B)^T
  \right] (\nu + n_B)^{-1}
  \label{eq:Sigma_B}
\end{equation}
%
where $\nu$ ($\nu>k-1$) is an additional free parameter (from the Inverse-Wishart prior on $\Sigma_B$) weighting the importance of $\Sigma_{D}$. When the target category has no members (i.e., $n_B = 0$), items are generated at random.

Generated exemplars are drawn from a multivariate Normal distribution specified by $(\mu_{B}, \Sigma_{B})$. Thus, $p(y)$ is
\begin{equation}
  p(y) = \dfrac
    {\exp( \theta \cdot {\rm Normal}(y; \mu_{B}, \Sigma_{B}))}
    {\sum_i \exp( \theta \cdot {\rm Normal}(y_i; \mu_{B}, \Sigma_{B}))} 
\end{equation}
where $\theta$ is a response determinism parameter and ${\rm Normal}(y; \mu, \Sigma)$ denotes a multivariate Normal density evaluated at $y$. 

\section{Behavioral Experiment}

\begin{figure}
    \begin{center}
    \input{figs/middle-bottom-conditions.pgf}
    \caption{Conditions tested in the behavioral experiment.}
    \label{fig:middle-bottom-conditions}
    \end{center}
\end{figure}


The copy-and-tweak and hierarchical sampling models were designed to explain effects of prior knowledge on the structure of categories, but they do not make any assumptions about the role of between-category contrast. Indeed, when there are no known examples of the target category, both models assume that generation is random. PACKER is thus unique in its prediction that contrast categories should influence both the structure and location of generated categories. The behavioral experiment described below was designed to test this key prediction. 

The experiment follows the paradigm developed by \citet{jern2013probabilistic}: first, participants learn members of a known category (`Alpha', or `A'), and are then asked to generate exemplars belonging to a new category (`Beta', or `B'). We developed two Alpha categories (see Figure \ref{fig:middle-bottom-conditions}): the `Bottom' Alpha category is a tight cluster in the bottom-center of the space, and the `Middle' Alpha category is identical except that it lies in the center of stimulus space.

Although our manipulation is minimal, the PACKER model predicts strong between-condition differences. According to PACKER, the nature of the space not occupied by the Alpha category should determine where members of the Beta category are likely to be generated. Thus, the lower areas of the stimulus space should be less frequently used for generation in the Bottom condition compared to the Middle (as these areas possess greater similarity to the Bottom Alpha category). Conversely, the upper areas of the stimulus space should be used for generation more frequently in the Bottom condition compared to Middle.


More generally, PACKER proposes that the probability a stimulus $y$ will be generated is a function of its similarity to contrast categories \textit{and} to members of the target category. Two more general predictions (not specific to either condition) follow from this proposal: (1) the location of Beta examples should be positively related to distance from the Alpha category, and (2) Beta examples should be more similar to one another than they are to members of the Alpha category.

\begin{figure}
    \begin{center}
    \inputpgf{figs/}{beta.samples.pgf}
    \caption{Sample generated categories. }
    \label{fig:beta.samples}
    \end{center}
\end{figure}

\subsubsection{Participants \& Materials}
We recruited 122 participants from Amazon Mechanical Turk from the US equally assigned to each condition. Stimuli were squares varying in color (grayscale 9.8\%-90.2\%) and size (3.0--5.8cm). The assignment of perceptual features (color, size) to axes of the domain space (x, y) was counterbalanced across participants.


\subsubsection{Procedure}

Participants began the experiment with a short training phase (3 blocks of 4 trials), where they observed exemplars belonging to the `Alpha' category. Participants were instructed to learn as much as they can about the Alpha category, and that they would answer a series of test questions afterwards. On each trial, a single Alpha category exemplar was presented, and participants were given as much time as they desired before moving on. Exemplars were randomly ordered within each block. Participants were shown the range of possible colors and sizes prior to training.

Following the training phase, participants were asked to generate four examples belonging to another category called `Beta'. Participants were instructed that members of the Beta category could be quite similar or different depending on what they think makes the most sense for the category, but that they were not allowed to make the same example twice. As in \citet{jern2013probabilistic}, generation was completed using a sliding-scale interface. Two scales controlled the features of the generated example. An on-screen preview of the example updated whenever one of the features was changed. Participants could generate any example along an evenly-spaced 9x9 grid, except for any previously generated Beta exemplars. Neither the members of the Alpha category nor the previously generated Beta examples were visible during generation. 

\subsubsection{Results}

Several sample Beta categories are depicted in Figure \ref{fig:beta.samples}. Because the conditions differ only in their location along the y-axis, we first focus on how Beta exemplars are generated above and below the contrast category. As is evident in Figure \ref{fig:beta.samples}, we observed broad individual differences in generation strategy: Whereas some participants generated all four Beta examples within a narrow y-axis range, others generated Beta examples along a wide range. 

To evaluate the key predictions of PACKER, we determined the number of participants in each condition who placed at least one Beta exemplar on the top and bottom `rows' of the space (the maximum and minimum possible y-axis value, respectively). The resulting contingencies data are shown in Table \ref{table:subset-table}. Fisher's Exact Tests reveal that more Middle participants generated a Beta exemplar in the bottom row , $p < 0.001$, but the conditions did not differ in use of the top of the space, $p = 0.16$. More Middle participants placed Beta exemplars in the top \textit{and} bottom rows, $p = 0.038$. 

\begin{table}
\begin{center} 
\caption{Behavioral results.} 
\label{table:subset-table} 
\vskip 0.12in
\begin{tabular}{ l r r}
    \textbf{Middle}         & Used top row & No top row \\
    \hline
    Used bottom row       &  28 & 18  \\
    No bottom row          &  11 &  4  \\
    \\
    \textbf{Bottom}         & Used top row & No top row \\
    \hline
    Used bottom row        & 16 & 8 \\
    No bottom row          & 31 & 6 \\
\end{tabular}
\end{center} 
\end{table}

To evaluate PACKER's other predictions, we computed the number of exemplars produced at different distances to the center of the Alpha category. These data (Figure \ref{fig:distance.figs} left) reveal a strong preference for stimuli that are dissimilar to the Alpha category members: maximally distant items were by far the most frequently generated. 

Finally, we computed for each participant the average distance between exemplars belonging to the same and opposite categories. These data (Figure \ref{fig:distance.figs} right) show that, as observed by \citet{ward1994structured}, most people generated Beta categories in which members are closer to one another than they are to members of the Alpha category (i.e., more between- than within-category distance). We did however, observe a notable subset of individuals with greater within-class distance. These individuals tended to adopt a `corners' approach, in which Beta examples were placed almost exclusively in the corners of the space.


\subsection{Summary}

Our results support PACKER's predictions: People tend to generate items that are dissimilar from the contrast category and similar to the target category. We observed considerable differences in generation between the Middle and Bottom conditions: Participants in the Bottom condition were less likely to use the bottom row of the stimulus space for generation, and participants in the Middle condition were more likely to create categories spanning the entire y-axis (utilizing the top and bottom row of the space). This latter result is especially interesting as it conflicts with previous results: Qualitatively different types of categories were generated, depending only on the location of the Alphas. 

Some aspects of the results described above are somewhat commonsense: They demonstrate that the location of existing categories imposes constraints on generation because people tend to generate examples in areas not occupied by existing categories. This principle, however, is novel and not predicted by existing models of generation -- these models were designed to explain distributional correspondences between generated and existing categories, not effects of contrast.  

\begin{figure}
    \begin{center}
    \inputpgf{figs/}{distance.figs.pgf}
    \caption{Behavioral results. \textit{Left}: Frequency of exemplar generation as a function of distance from the Alpha category normalized by the maximum possible distance. \textit{Right}: Within- vs. between-category distance for every participant. }
    \label{fig:distance.figs}
    \end{center}
\end{figure}


\section{Model Evaluation}
To obtain an overall sense of each model's ability to explain our results, we fit each model by maximizing the log-likelihood of the model's predictions of the human results. The $c$, $\gamma$, and $\theta$ parameters were fitted for PACKER; $c$, and $\theta$ were fitted for the copy-and-tweak model ($\gamma$ was fixed at 1), and $\kappa$, $\lambda$, $\nu$, and $\theta$ were fitted for the hierarchical sampling model. Note that each model possess a $\theta$ parameter fulfilling the same role (response determinism). Attention in PACKER and copy-and-tweak was set uniformly. Parameters were not allowed to vary between participants or conditions -- the goal was to obtain the best-fitting values to our entire dataset.

Each model's best-fitting parameterization is shown in Table \ref{table:model-fits}. Overall, PACKER outperformed copy-and-tweak and the hierarchical sampling model by a considerable margin ($\sim13\%$ improvement in log-likelihood). The parameter settings associated with PACKER's best fit are exactly as expected: a strong preference for items that are similar to members of the target category but are dissimilar to members of the contrast category. A similar pattern of results was obtained when we only considered the second to fourth exemplars generated by each participant.

Our model-fitting results make sense given the assumptions made by each model. As the copy-and-tweak and hierarchical sampling models are not influenced by the location of contrast categories within the space, they do not capture the broad tendency for generated items to be dissimilar to existing classes. 


\subsection{Relation Between Category Structure \& Location}

Generally, our behavioral results showed that members of generated categories are dissimilar to opposite categories, and similar to members of their own category. However, we also observed a great deal of individual differences in generation style. Manually inspecting the data reveals four typical patterns (see Figure \ref{fig:beta.samples}): `corners' categories with one Beta example in each corner of the space, tight clusters, `column'-like categories, and `row'-like categories. This informal inspection also reveals that each of these category types tended to be generated into distinct regions of the domain, suggesting a link between category location and distributional structure.

\begin{table}
\centering
\caption{Model-fitting results.}
\label{table:model-fits}
\begin{tabular}{ l l l}
\\
 \textbf{PACKER} & \textbf{Copy \& Tweak} & \textbf{Hierarchical} \\
 & & \textbf{Sampling} \\ \hline
 $AIC = 3474$ & $AIC = 3914$ & $AIC = 3972$  \\ 
 $c=0.565$      & $c=4.894$  & $\kappa<0.001$\\
 $\gamma=0.469$ & $\gamma=1$ (fixed) & $\nu=4.660$ \\ 
 $\theta=6.632$ & $\theta=3.712$ &  $\lambda=0.423$  \\ 
  &  & $\theta = 2.771$  \\ 
\end{tabular}
\end{table}

\begin{figure*}
    \begin{center}
    \inputpgf{figs/}{range-diff-gradients.pgf}
    \caption{Generated category structure as a function of location. Orange areas in each gradient correspond to stimuli that were commonly generated into category possessing greater y-axis range (columns). Purple areas correspond to categories possessing greater x-axis range. White areas correspond to equal range along both features (or infrequent generation).}
    \label{fig:range-diff-gradient}
    \end{center}
\end{figure*}

To more systematically evaluate this possibility, we computed, for each stimulus in the domain, the difference in range between the features ($range(size) - range(color)$) across every generated category that had the stimulus as member. Aggregating over these range differences yields a gradient describing how categories tended to distributed for each stimulus. These data (Figure \ref{fig:range-diff-gradient}) reveal a systematic relationship between category structure and location. Whereas column-like categories more often include stimuli to the left or right of the Alpha class, row-like categories appear above and below the Alpha class.  Thus, participants modify the distributional structure of new categories to the maximize distance from the contrast category.

To simulate this finding, we set the attention weight parameters in PACKER and copy-and-tweak per participant. The other free parameters were set as in Table \ref{table:model-fits}. While there exist methods to find the optimal attention weights for a given classification \citep[see][]{vanpaemel2012using}, for simplicity we approximated the weights using proportionally to inverse of each feature's range: Thus, the Alpha and Beta categories are assumed to be distinct along dimensions that the Betas do not vary on. To simulate the hierarchical sampling model we set the domain covariance prior, $\Sigma_0$, proportional to the range (not inverted) of each feature: Thus new categories were distributed more widely along the features that each participant used more widely. We then simulated 50 Beta categories with each participant's weighting scheme to obtain a sense of how the relative importance of each dimension affects what types of categories are generated and where they are generated. The results of these simulations are depicted in Figure \ref{fig:range-diff-gradient}.

When the x-axis is weighted more, PACKER creates column categories to the sides of the Alphas. Conversely, when the y-axis is weighted more, PACKER creates row categories above and below the Alphas. This behavior falls out from the nature of selective attention: Dimensions weighted more have a sharper similarity gradient. For example, when the x-axis is weighted more, PACKER favors Beta categories with more within-class similarity (less range), and less between-class similarity along the x-axis, resulting in column-like categories that differ from the Alphas along the x-axis.

Although differentially weighting the features results in different types of categories from the hierarchical sampling and copy-and-tweak models, the location of the Alpha category does not affect where items will be generated by these models. Thus, row- and column-like categories are not systematically generated in different areas of the stimulus space, resulting in the uniform predictions shown in Figure \ref{fig:range-diff-gradient}.

\section{Discussion}
The creative use of conceptual knowledge is a fascinating yet understudied topic in categorization. In this paper, we presented a novel exemplar-based approach to explaining category generation. The PACKER model proposes that categories are represented as a collection of exemplars stored in memory, and that members of generated categories should be similar to one another, yet dissimilar to members of opposing categories. Exemplar models can be viewed as Importance-Sampling approximations of Bayesian models \citep{shi10}. So, PACKER can be viewed as a rational process model, approximating the expected density of a new category based on a contrast category. 

In a behavioral study and subsequent formal modeling, we found broad support for the PACKER model. Participants in our study more frequently generated items that are distant from members of contrast categories, and they tended to generate categories with more within-class than between-class similarity. Likewise, we found that the \textit{location} of contrast categories (as opposed to their structure) shapes generation by imposing constraints on the areas of space that remain available for a new category. Formal simulations reveal that existing models \citep[see][]{jern2013probabilistic}, making no assumptions about category-contrast, do not account for these effects.

The PACKER model is, in general, highly expressive in its performance. Under different parameter settings it is capable of generating tightly clustered or highly distributed categories, and adjusting the distribution of categories along each feature. Future work will focus on exploring the broad degree of individual differences we observed in generation, and whether PACKER can explain previous results in the field \citep{ward1994structured,jern2013probabilistic}.

\section{Acknowledgments}
Support for this research was provided by the Office of the VCRGE at the UW - Madison with funding from the WARF. We thank Kenneth Kurtz for helpful comments, and Alan Jern and Charles Kemp for providing code and data.


\bibliographystyle{apacite}
\setlength{\bibleftmargin}{.025in}
\setlength{\bibindent}{-\bibleftmargin}
\bibliography{references}

\end{document}
