%!TEX output_directory = latex_out/

\documentclass[12pt]{article}
\usepackage[letterpaper, margin=1in, headheight=15pt]{geometry}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{csquotes}
\usepackage{fancyvrb}
\usepackage[natbibapa]{apacite}


\begin{document}

\section{The Copy-And-Tweak Model}

This model, initially implemented by \cite{jern2013probabilistic} to explain exemplar generation in a multinomial space, is broadly based on Ward's path-of-least-resistance model \citep{ward1994structured,ward1995s,ward2002role}, but is quite different in implementation. This document describes how to implement their copy-and-tweak approach within our 2D continuous domain. See Appendix \ref{ap:jk13ap} for description of the model provided by \cite{jern2013probabilistic}.

The model proposes that, when prompted to generate new examples, people copy a known example from the target class and change it a small amount along each dimension. The core difference between this account and the hierarchical sampling account is that correlations between features are not maintained by a copy-and-tweak system. As a result, this issue was the focus of \cite{jern2013probabilistic}.

In order to generate a new exemplar, the model computes the similarity between the members of the target category $x$and possible generation option $y$. Similarity $s$ between two examples, $x_i$ and $y_i$, is an inverse exponential function of distance:

\begin{equation}
  s(x_i,y_i) = exp( -c \sum_k{|x_{ik} - y_{ik}|w_k})
\end{equation}

Where $c$ and $w_k$ are the standard specificity and attention weights discussed elsewhere \citep[see][]{nosofsky1984choice}.

\begin{equation}
  s(x_i,y_i) = \sum_i{s(y, x_i)}
\end{equation}

The probability that a given candidate $y$ will be generated is based on its summed similarity to all members of the target category $x$: $\sum_i{s(y, x_i)}$.
\begin{equation}
    p(y)  = \dfrac
    { \exp \left\{\theta \cdot \sum_i{s(y, x_i)} \right\} }
    {\sum_j{\exp \left\{ \theta \cdot \sum_i{s(y_j, x_i)} \right\} }} 
    \label{eq:generation-choice}
\end{equation}
% 
where $\theta$ is a response determinism parameter. Known members of the target category cannot be generated, and so their probability is manually set to 0. If there are no known members of the target category, generation is random (uniform probabilities over $y$).

\subsection{Parameter info}
\begin{itemize}
    \setlength\itemsep{-0.5em}
    \item $c$. Scalar, $c>0$. Specificity parameter.
    \item $\theta$. Scalar, $\theta > 0$. Response determinism parameter.
\end{itemize}



% references section
\clearpage
\bibliographystyle{apacite}
\bibliography{citations.bib}


\clearpage
\appendix
\section{Description from \cite{jern2013probabilistic}}
\label{ap:jk13ap}

\subsection{Passages from the Introduction}


\subsubsection{Page 93}
\begin{displayquote}
Exemplar models like the GCM (Nosofsky, 1986) and ALCOVE (Kruschke, 1992) classify novel exemplars by computing their similarity to previously observed exemplars stored in memory. These models can be extended to account for generation by assuming that stored exemplars can be copied and changed in some way in order to produce novel exemplars. We will refer to the resulting approach as the copy-and-tweak account, and the same basic approach has been proposed by several previous authors (Barsalou \& Prinz, 1997; Barsalou, 1999; Ward, 1995; Ward, Patterson, Sifonis, Dodds, \& Saunders, 2002; Ward, Patterson, \& Sifonis, 2004). 

Although the copy-and-tweak account may seem conceptually different from the sampling account, in some cases the two approaches are equivalent. Consider, for example, a problem in which the exemplars are points in n-dimensional space. A copy-and-tweak account would retrieve one of these exemplars and change it slightly along each dimension. This procedure is conceptually identical to sampling from a sum of normal distributions centered on each observed exemplar, which is formally equivalent to the GCM (Ashby \& Alfonso-Reese, 1995; Jäkel, Schölkopf, \& Wichmann, 2008). Thus, in many cases, applying copy-and-tweak to an exemplar model will be equivalent to sampling from the generation distribution learned by a generative model. The relationship between copy-andtweak and the sampling account suggests that both accounts will make similar predictions for problems in which people generate from categories that correspond to clusters in a similarity space. The two accounts, however, will diverge in cases where category membership cannot naturally be captured by similarity. Our first set of experiments uses structured categories that are specifically designed to distinguish between the two accounts.
\end{displayquote}

\subsubsection{Pages 103-104}
\begin{displayquote}
As described earlier, the copy-and-tweak approach converts an exemplar model of classification into a model of generation. The approach proposes that new exemplars are generated by copying one of the exemplars stored in memory and then modifying some features of the copy. When we introduced the category used for Experiments 1 and 2, we noted that our stimuli were carefully chosen to distinguish between the structured sampling account and the copy-and-tweak account. The structured sampling model is capable of learning that pairs of letters are grouped into cohesive parts and therefore predicts that people will tend to generate novel combinations of parts. In contrast, a copy-and-tweak model will generate many exemplars that are not valid exemplars. For example, after observing the four exemplars in Fig. 4b, a copy-and-tweak model might produce a new exemplar by copying the first exemplar and changing the top letter from D to X.

We implemented a copy-and-tweak model based largely on the GCM (Nosofsky, 1986). The GCM makes classification judgments by computing the similarity between a novel exemplar and all of the stored exemplars, and similarity is computed as a function of pairwise distance between exemplars. Because our stimuli used discrete non-ordinal features, we used the Hamming distance, which measures the number of features that are different. As explained earlier, the GCM is equivalent to a generative model that maintains a probability distribution with peaks at the observed exemplars (Ashby \& Alfonso-Reese, 1995; Jakel et al., 2008). The generative model that corresponds to the GCM maintains a probability distribution over a continuous similarity space, and we implemented an analogous approach that maintains a probability distribution over our space of discrete flu genomes.
\end{displayquote}


\subsection{Their appendix}

\begin{displayquote}

The copy-and-tweak model uses similarity to observed exemplars as the basis for both classification and generation. We used a similarity rule derived from the GCM (Nosofsky, 1986) that defines the similarity $s$ between exemplars $x_1$ and $x_2$ as $s(x_1,x_2) = \exp (-c \cdot d(x_1,x_2))$, where $c$ is a scaling parameter and $d(x_1,x_2)$ is the distance between the two exemplars. We set $c$ to 1 and used the Hamming distance, which counts the number of features for which the two exemplars have different values.

To derive classification predictions from the copy-and-tweak model for the rating task, we summed the similarities of each rating exemplar $x_r$ to each of the training exemplars $x_i$: $\text{rating}(x_r) =\sum_i{s(x_r,x_i)}$. We then applied a z-score transformation to produce the predictions in Fig. 10. 

To derive generation predictions from the copy-and-tweak model, we computed similarity ratings for all possible exemplars and drew 2,000,000 samples from a Multinomial distribution based on the ratings:

\begin{equation}
  \widetilde{x} \sim \text{Multinomial}([R \cdot \text{rating}(\widetilde{x}_1) ... \text{rating}(\widetilde{x}_n)])
\end{equation}
% 
where $R = 1/\sum_i{\text{rating}(\widetilde{x}_i)}$ is a normalization constant. The reported predictions are based on a model which considers all possible exemplars that can be constructed from the 16 observed letters ($n = 16^4$ ). We also implemented an alternative model that considers only exemplars in which each letter can only appear in positions where it has been previously observed (n = 256). The two models produced similar predictions, but the former version performed slightly better.
\end{displayquote}

\end{document}
